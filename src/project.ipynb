{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import  StandardScaler, MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from utils.data_preparation import *\n",
    "from utils.data_exploration import *\n",
    "\n",
    "from rdkit import Chem\n",
    "\n",
    "from feature_engine.selection import DropCorrelatedFeatures\n",
    "\n",
    "\n",
    "task = 'CYP2C19'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Dataset + Data Cleaning\n",
    "\n",
    "In terms of data cleaning the following steps are performed:\n",
    "\n",
    "- Normalization of smiles strings before calculating descriptors and fingerprints\n",
    "  - Normalization includes the removal of metals in the molecule (<span style=\"color:cyan\">TODO</span> Why?)\n",
    "- Removal of small molecules\n",
    "  - For example: \n",
    "    - Molecules consisting of a single atom (<span style=\"color:cyan\">TODO</span> Why?)\n",
    "    - Molecules that are metals\n",
    "- *Molecular Descriptors:* Removing of NaN values by either removing the corresponding column or row. \n",
    "  - For molecular descriptors it doesn't make much sense to fill missing values with some default value or mean of the existing values\n",
    "\n",
    "\n",
    "In the first iteration we will focus on using the Morgan fingerprints. If there is time later we will explore other fingerprints and compare. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_preprocessing(task)\n",
    "# data = remove_small_molecules(data)\n",
    "data = data.drop([\"MACCS_FP\", \"ATOMPAIR_FP\"], axis=1)\n",
    "\n",
    "# turn string of fingerprints into single features\n",
    "morgan_fingerprint_df = pd.DataFrame(convert_strings_to_int_array(data[\"Morgan_FP\"].values), index=data.index)\n",
    "data = data.merge(morgan_fingerprint_df, left_index=True, right_index=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove missing values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_nan = extract_null(data)\n",
    "data_nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"There are {data_nan.shape[0]} ({data_nan.shape[0]/data.shape[0]*100:.2f}%) molecules and {data_nan.shape[1]-3} descriptors with missing values.\"\n",
    ")\n",
    "summarize_descriptors(data_nan.columns[3:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since only 0.28% of molecules have missing values we simply remove those molecules since using a default value doesn't make much sense for the shown descriptors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(data_nan.index)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data in train, val, test\n",
    "datasets = dataset_split(data.drop([\"Drug\", \"Drug_ID\"], axis=1))\n",
    "datasets[\"train\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Exoploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets[\"train\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_counts(\n",
    "    [datasets[\"train\"][\"Y\"], datasets[\"val\"][\"Y\"], datasets[\"test\"][\"Y\"]],\n",
    "    suptitle=\"Distribution of the target label within each set\",\n",
    "    titles=[\"train\", \"validation\", \"test\"],\n",
    "    legend_title=\"CYP2C19 inhibition\",\n",
    "    kind=\"pie\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The descriptors include discrete and continuous data, distinguished by their dtype. \n",
    "\n",
    "unique_dtypes = set(datasets[\"train\"].dtypes)\n",
    "print(f\"Datatypes: {unique_dtypes}\")\n",
    "\n",
    "continuous_descriptors = list(datasets[\"train\"].select_dtypes(include='float64').columns)\n",
    "discrete_descriptors = list(datasets[\"train\"].select_dtypes(include='int64').columns)\n",
    "fingerprint_features = list(morgan_fingerprint_df.columns)\n",
    "for fingerprint_feature in fingerprint_features:\n",
    "    discrete_descriptors.remove(fingerprint_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuous Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_distributions(\n",
    "    data = datasets[\"train\"][[\"Y\"]+continuous_descriptors],\n",
    "    features = continuous_descriptors[10:18],\n",
    "    suptitle=\"Feature distributions given the target label using a KDE\",\n",
    "    task=f\"{task} inhibition\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix of descriptors\n",
    "cor_matrix =  datasets[\"train\"][continuous_descriptors].corr()\n",
    "top_cor_matrix = cor_matrix.where(np.triu(np.ones(cor_matrix.shape),k=1).astype(np.bool))\n",
    "fig = px.imshow(top_cor_matrix, color_continuous_scale='RdBu_r', title=f\"{task} inhibition\\nDescriptor correlation\")\n",
    "\n",
    "fig.write_html(f\"data/{task.lower()}/descriptor_correlation.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discrete Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_distributions(\n",
    "    data=datasets[\"train\"][discrete_descriptors],\n",
    "    features=discrete_descriptors[5:11],\n",
    "    kind=\"hist\",\n",
    "    suptitle=\"Feature Distributions given the target label\",\n",
    "    task=\"CYP2C19 inhibition\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "\n",
    "There are in total 208 different descriptors. Relevant descriptors for the task of predicting CYP inhibition need to be selected to reduce the number of input variables to the clasical machine learning algorithm. Feature selection can either be performed unsupervised (without knowledge of the target label) or supervised.\n",
    "\n",
    "**Note:** Some machine learning models have some form of feature selection inbuild, e.g. tree-based models. In those case we don't perform feature selection upfront.\n",
    "\n",
    "Having a look at for example the number of radical electrons (NumRadicalElectrons) (<span style=\"color:cyan\">TODO</span> Add description of NUmRadicalElectrons). We can see that all datapoints in the dataset have a value of 0 (min=max=0.0). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(summarize_descriptors([\"NumRadicalElectrons\"]))\n",
    "datasets[\"train\"].describe()[\"NumRadicalElectrons\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the area of feature selection there is a method called **variance threshold**: Given a threshold all features with a variance below this threshold will be removed. (<span style=\"color:cyan\">TODO</span> Add better source; https://medium.com/nerd-for-tech/removing-constant-variables-feature-selection-463e2d6a30d9#:~:text=Variance%20Threshold%20is%20a%20feature,be%20used%20for%20unsupervised%20learning.)\n",
    "\n",
    "The default value is usually 0 (removing constant features as they obviously bring no additional information to our model). If the variance threshold is greater than zero but still small we are removing quasi-constant features. The arguments against using a variance greater than 0 say that you may be moving variables that, although they have low variance, might actually be extremely powerful in explaining your target (dependent) variable.\n",
    "\n",
    "For now, we are exploring which features are constant in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Features with 0 variance:\\n\")\n",
    "for index, n_unique in zip(datasets[\"train\"].nunique(axis=0).index, datasets[\"train\"].nunique(axis=0)):\n",
    "    if n_unique == 1:\n",
    "        print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline(steps=[\n",
    "    (\"remove_zero_var\", VarianceThreshold(threshold=0.0))])\n",
    "\n",
    "# zero-variance + correlation (between features) PCA, mutual information (discrete and continuous)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in the correlation matrix there are some feature groups in our dataset with high correlation. In order to escape the curse of dimensionality we want to remove features with a high correlation to other features - out of two features with high correlation only one remains. When features are collinear, permutating one feature will have little effect on the models performance because it can get the same information from a correlated feature. One way to handle multicollinear features is by performing hierarchical clustering on the Spearman rank-order correlations, picking a threshold, and keeping a single feature from each cluster. Source: https://scikit-learn.org/stable/auto_examples/inspection/plot_permutation_importance_multicollinear.html\n",
    "\n",
    "The y-axis of the following dendrogram is a measure of closeness of either individual data points or clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dendrogram(cor_matrix, level=7, color_threshold=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea of dropping highly correlated features is also applied by the following method: [DropCorrelatedFeatures](https://feature-engine.readthedocs.io/en/1.1.x/selection/DropCorrelatedFeatures.html) from the feature_engine. Here, features are removed on first found first removed basis, without any further insight using pearson correlation score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring DropCorrelatedFeatures\n",
    "\n",
    "drop_corr_features = DropCorrelatedFeatures(threshold=0.8)\n",
    "print(\"Number of features before transformation:\", datasets[\"train\"][continuous_descriptors].shape[1])\n",
    "reduced_continuous_data = drop_corr_features.fit_transform(datasets[\"train\"][continuous_descriptors])\n",
    "print(\"Number of features after transformation:\", reduced_continuous_data.shape[1])\n",
    "\n",
    "# Correlation matrix of descriptors\n",
    "reduced_cor_matrix = reduced_continuous_data.corr()\n",
    "reduced_top_cor_matrix = reduced_cor_matrix.where(\n",
    "    np.triu(np.ones(reduced_cor_matrix.shape), k=1).astype(np.bool)\n",
    ")\n",
    "fig = px.imshow(\n",
    "    reduced_top_cor_matrix,\n",
    "    color_continuous_scale=\"RdBu_r\",\n",
    "    title=f\"{task} inhibition\\nDescriptor correlation after dropping highly correlated features\",\n",
    ")\n",
    "\n",
    "fig.write_html(f\"data/{task.lower()}/descriptor_correlation_pruned.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline(\n",
    "    steps=[\n",
    "        (\n",
    "            \"continuous-drop_corr_features\",  # this needs to be first since it takes a DataFrame as an input\n",
    "            DropCorrelatedFeatures(\n",
    "                variables=continuous_descriptors,\n",
    "                threshold=0.8,\n",
    "            ),\n",
    "        ),\n",
    "        (\"discrete-drop_zero_var\", VarianceThreshold(threshold=0.0)),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality reduction\n",
    "\n",
    "For continuous data we will perform a PCA to reduce the dimensionality of the features. Since PCA should only be applied to continuous data we will split our preprocessing pipeline into three parts:\n",
    "\n",
    "1. Preprocessing of continuous descriptors\n",
    "2. Preprocessing of discrete descriptors\n",
    "3. Preprocessing of the fingerprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous_preprocessing = Pipeline(\n",
    "    steps=[  # DropCorrelatedFeatures needs to be first since it takes a DataFrame as an input\n",
    "        (\n",
    "            \"drop_corr_features\",\n",
    "            DropCorrelatedFeatures(\n",
    "                variables=continuous_descriptors,\n",
    "                threshold=0.8,\n",
    "            ),\n",
    "        ),\n",
    "        (\"drop_zero_var\", VarianceThreshold(threshold=0.0)),\n",
    "        (\n",
    "            \"normalize\",\n",
    "            StandardScaler(),\n",
    "        ),  # pca assumes mean=0 and variance=1\n",
    "        (\"pca\", PCA(n_components=70)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "discrete_preprocessing = Pipeline(\n",
    "    steps=[\n",
    "        (\"drop_zero_var\", VarianceThreshold(threshold=0.0)),\n",
    "        (\"min_max_normalization\", MinMaxScaler()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "fingerprint_preprocessing = Pipeline(\n",
    "    steps=[\n",
    "        (\"drop_zero_var\", VarianceThreshold(threshold=0.0)),\n",
    "        (\"min_max_normalization\", MinMaxScaler()),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fingerprint_preprocessing.fit(datasets[\"train\"][fingerprint_features], datasets[\"train\"][\"Y\"])\n",
    "discrete_preprocessing.fit(datasets[\"train\"][discrete_descriptors], datasets[\"train\"][\"Y\"])\n",
    "continuous_preprocessing.fit(datasets[\"train\"][continuous_descriptors], datasets[\"train\"][\"Y\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Steps\n",
    "\n",
    "- Use Dendrogram [only continuous data] for feature selection (Jonna)\n",
    "- Feature selection method for discrete data (James)\n",
    "- Baseline\n",
    "  - DummyClassifier\n",
    "- Classical models\n",
    "  - Random Forest (little feature selection) (Jonna)\n",
    "  - KNN (James)\n",
    "  - XGBoost (James)\n",
    "  - SVM (Jonna)\n",
    "  - Linear Models (Jonna)\n",
    "  - Naive Bayes (James)\n",
    "  - [later] simple NN\n",
    "- Try giving weights to classes (will solve unbalanced data sets) (James)\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('astrazeneca')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "edc3225ee2ce0e02b9c5eb90a1bbc23c3d648c7d148b1d7ba33570195183ff0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
