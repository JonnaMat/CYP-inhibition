{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from feature_engine.selection import DropCorrelatedFeatures\n",
    "\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "from skopt.plots import plot_evaluations, plot_convergence, plot_objective\n",
    "from skopt.utils import dump, load\n",
    "\n",
    "from utils.data_preparation import *\n",
    "from utils.data_exploration import *\n",
    "from utils.training import *\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "task = \"cyp2c19\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Dataset + Data Cleaning\n",
    "\n",
    "In terms of data cleaning the following steps are performed:\n",
    "\n",
    "- Normalization of smiles strings before calculating descriptors and fingerprints\n",
    "  - Normalization includes the removal of metals in the molecule (<span style=\"color:cyan\">TODO</span> Why?)\n",
    "- Removal of small molecules\n",
    "  - For example: \n",
    "    - Molecules consisting of a single atom (<span style=\"color:cyan\">TODO</span> Why?)\n",
    "    - Molecules that are metals\n",
    "- *Molecular Descriptors:* Removing of NaN values by either removing the corresponding column or row. \n",
    "  - For molecular descriptors it doesn't make much sense to fill missing values with some default value or mean of the existing values\n",
    "\n",
    "\n",
    "In the first iteration we will focus on using the Morgan fingerprints. If there is time later we will explore other fingerprints and compare. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_preprocessing(task)\n",
    "data = data.drop([\"MACCS_FP\", \"ATOMPAIR_FP\"], axis=1)\n",
    "data = select_druglike_molecules(data)\n",
    "# data = remove_small_molecules(data)\n",
    "\n",
    "# turn string of fingerprints into single features\n",
    "morgan_fingerprint_df = pd.DataFrame(\n",
    "    convert_strings_to_int_array(data[\"Morgan_FP\"].values), index=data.index\n",
    ")\n",
    "data = data.merge(morgan_fingerprint_df, left_index=True, right_index=True)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove missing values \n",
    "Since less than 1% of molecules have missing values we simply remove those molecules since using a default value doesn't make much sense for the shown descriptors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_nan = extract_null(data)\n",
    "print(\n",
    "    f\"There are {data_nan.shape[0]} ({data_nan.shape[0]/data.shape[0]*100:.2f}%) molecules and {data_nan.shape[1]-3} descriptors with missing values.\"\n",
    ")\n",
    "data_nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(data_nan.index)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Validation-Test split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data in train, val, test\n",
    "datasets = dataset_split(data.drop([\"Drug\", \"Drug_ID\", \"Morgan_FP\"], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The descriptors include discrete and continuous data, distinguished by their dtype.\n",
    "\n",
    "feature_groups = get_feature_groups(datasets, morgan_fingerprint_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Exoploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets[\"train\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_counts(\n",
    "    [datasets[\"train\"][\"Y\"], datasets[\"val\"][\"Y\"], datasets[\"test\"][\"Y\"]],\n",
    "    suptitle=\"Distribution of the target label within each set\",\n",
    "    titles=[\"train\", \"validation\", \"test\"],\n",
    "    legend_title=\"CYP2C19 inhibition\",\n",
    "    kind=\"pie\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuous Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_distributions(\n",
    "    data=datasets[\"train\"][[\"Y\"] + feature_groups.continuous],\n",
    "    features=feature_groups.continuous[10:14],\n",
    "    suptitle=\"Feature distributions given the target label using a KDE\",\n",
    "    task=f\"{task} inhibition\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix of descriptors\n",
    "cor_matrix = datasets[\"train\"][feature_groups.continuous].corr()\n",
    "top_cor_matrix = cor_matrix.where(\n",
    "    np.triu(np.ones(cor_matrix.shape), k=1).astype(np.bool)\n",
    ")\n",
    "fig = px.imshow(\n",
    "    top_cor_matrix,\n",
    "    color_continuous_scale=\"RdBu_r\",\n",
    "    title=f\"{task} inhibition\\nDescriptor correlation\",\n",
    ")\n",
    "\n",
    "fig.write_html(f\"data/{task.lower()}/descriptor_correlation.html\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discrete Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_distributions(\n",
    "    data=datasets[\"train\"][[\"Y\"] + feature_groups.discrete],\n",
    "    features=feature_groups.discrete[5:9],\n",
    "    kind=\"hist\",\n",
    "    suptitle=\"Feature Distributions given the target label\",\n",
    "    task=\"CYP2C19 inhibition\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "\n",
    "There are in total 208 different descriptors. Relevant descriptors for the task of predicting CYP inhibition need to be selected to reduce the number of input variables to the clasical machine learning algorithm. Feature selection can either be performed unsupervised (without knowledge of the target label) or supervised.\n",
    "\n",
    "**Note:** Some machine learning models have some form of feature selection inbuild, e.g. tree-based models. In those cases we don't perform feature selection upfront.\n",
    "\n",
    "### Variance Threshold\n",
    "\n",
    "Having a look at for example the number of radical electrons (NumRadicalElectrons). We can see that all datapoints in the dataset have a value of 0 (min=max=0.0). \n",
    "\n",
    "In the area of feature selection there is a method called **variance threshold**: Given a threshold all features with a variance below this threshold will be removed. (<span style=\"color:cyan\">TODO</span> Add better source; https://medium.com/nerd-for-tech/removing-constant-variables-feature-selection-463e2d6a30d9#:~:text=Variance%20Threshold%20is%20a%20feature,be%20used%20for%20unsupervised%20learning.)\n",
    "\n",
    "The default value is usually 0 (removing constant features as they obviously bring no additional information to our model). If the variance threshold is greater than zero but still small we are removing quasi-constant features. The arguments against using a variance greater than 0 say that you may be moving variables that, although they have low variance, might actually be extremely powerful in explaining your target (dependent) variable.\n",
    "\n",
    "For now, we are exploring which features are constant in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(summarize_descriptors([\"NumRadicalElectrons\"]))\n",
    "datasets[\"train\"][\"NumRadicalElectrons\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Features with 0 variance:\\n\")\n",
    "for index, n_unique in zip(\n",
    "    datasets[\"train\"].nunique(axis=0).index, datasets[\"train\"].nunique(axis=0)\n",
    "):\n",
    "    if n_unique == 1:\n",
    "        print(index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Correlated Features\n",
    "\n",
    "As shown in the correlation matrix there are some feature groups in our dataset with high correlation. In order to escape the curse of dimensionality we want to remove features with a high correlation to other features - out of two features with high correlation only one remains. When features are collinear, permutating one feature will have little effect on the models performance because it can get the same information from a correlated feature. One way to handle multicollinear features is by performing hierarchical clustering on the Spearman rank-order correlations, picking a threshold, and keeping a single feature from each cluster. Source: https://scikit-learn.org/stable/auto_examples/inspection/plot_permutation_importance_multicollinear.html\n",
    "\n",
    "The y-axis of the following dendrogram is a measure of closeness of either individual data points or clusters. \n",
    "\n",
    "\n",
    "The idea of dropping highly correlated features is also applied by the following method: [DropCorrelatedFeatures](https://feature-engine.readthedocs.io/en/1.1.x/selection/DropCorrelatedFeatures.html) from the feature_engine. Here, features are removed on first found first removed basis, without any further insight using pearson correlation score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dendrogram(cor_matrix, level=7, color_threshold=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring DropCorrelatedFeatures\n",
    "\n",
    "drop_corr_features = DropCorrelatedFeatures(threshold=0.8)\n",
    "print(\n",
    "    \"Number of features before transformation:\",\n",
    "    datasets[\"train\"][feature_groups.continuous].shape[1],\n",
    ")\n",
    "reduced_continuous_data = drop_corr_features.fit_transform(\n",
    "    datasets[\"train\"][feature_groups.continuous]\n",
    ")\n",
    "print(\"Number of features after transformation:\", reduced_continuous_data.shape[1])\n",
    "\n",
    "# Correlation matrix of descriptors\n",
    "reduced_cor_matrix = reduced_continuous_data.corr()\n",
    "reduced_top_cor_matrix = reduced_cor_matrix.where(\n",
    "    np.triu(np.ones(reduced_cor_matrix.shape), k=1).astype(np.bool)\n",
    ")\n",
    "fig = px.imshow(\n",
    "    reduced_top_cor_matrix,\n",
    "    color_continuous_scale=\"RdBu_r\",\n",
    "    title=f\"{task} inhibition\\nDescriptor correlation after dropping highly correlated features\",\n",
    ")\n",
    "\n",
    "fig.write_html(f\"data/{task.lower()}/descriptor_correlation_pruned.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Percentile\n",
    "\n",
    "For discrete features and fingerprints we are using a mutual information statistical test and apply multivariate feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality reduction\n",
    "\n",
    "### PCA\n",
    "\n",
    "For continuous data we will perform a PCA to reduce the dimensionality of the features. Since PCA should only be applied to continuous data we will split our preprocessing pipeline into three parts:\n",
    "\n",
    "1. Preprocessing of continuous descriptors\n",
    "2. Preprocessing of discrete descriptors\n",
    "3. Preprocessing of the fingerprint\n",
    "\n",
    "See DataPreprocessing in utils/training.py for the exact preprocessing pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Normalization\n",
    "\n",
    "For continuous features we are using StandardScaler as this is assumed by PCA. For discrete descriptors we are using a MinMaxScaler. Since fingerprint features are binary we don't normalize them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_baseline(datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVC\n",
    "\n",
    "To keep track of old Bayesian Optimization runs and their differences:\n",
    "\n",
    "#### SVC_bayesian_0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_bayesian_optimizer = BayesianOptimization(\n",
    "    model=SVC,\n",
    "    file_name=f\"{task}/svc_bayesian_0\", \n",
    "    model_params=[\n",
    "        Real(name=\"C\", low=0.1, high=4.0)\n",
    "    ],\n",
    "    datasets=datasets,\n",
    "    feature_groups=feature_groups,\n",
    ")\n",
    "\n",
    "# svc_results = svc_bayesian_optimizer.optimize() \n",
    "\n",
    "# LOAD results from bayesian optimization\n",
    "svc_results = pd.read_csv(f\"optimization/{task}/svc_bayesian_0\").drop(\"Unnamed: 0\", axis=1)\n",
    "\n",
    "svc_results.sort_values(\"val_accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_bayesian_optimizer.best_confusion_matrix(svc_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### svc_random_0\n",
    "\n",
    "We can do random search by setting n_calls = n_initial_points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_bayesian_optimizer = BayesianOptimization(\n",
    "    model=SVC,\n",
    "    file_name=f\"{task}/svc_random_0\", \n",
    "    model_params=[\n",
    "        Real(name=\"C\", low=0.1, high=4.0),\n",
    "    ],\n",
    "    datasets=datasets,\n",
    "    feature_groups=feature_groups,\n",
    ")\n",
    "\n",
    "# results = svc_bayesian_optimizer.optimize(n_calls=50, n_initial_points=50) \n",
    "\n",
    "# LOAD results from bayesian optimization\n",
    "# results = load(f\"optimization/{task}/svc_random_0\")\n",
    "\n",
    "# svc_bayesian_optimizer.best_confusion_matrix(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### svc_random_1_poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_random_1_poly_optimizer = BayesianOptimization(\n",
    "    model=SVC,\n",
    "    file_name=f\"{task}/svc_random_1_poly\", \n",
    "    model_params=[\n",
    "        Real(name=\"C\", low=0.1, high=4.0),\n",
    "        Categorical(name=\"kernel\", categories=[\"poly\"]),\n",
    "        Integer(name=\"degree\", low=3, high=12)\n",
    "    ],\n",
    "    datasets=datasets,\n",
    "    feature_groups=feature_groups,\n",
    "    preprocessing_params=[\n",
    "                Categorical(name=\"var_threshold_continuous\", categories=[0.0]),\n",
    "                Real(name=\"var_threshold_discrete\", low=0.02, high=0.05),\n",
    "                Categorical(name=\"var_threshold_fingerprint\", categories=[0.0]),\n",
    "                Real(name=\"corr_threshold\", low=0.7, high=0.95),\n",
    "            ]\n",
    ")\n",
    "\n",
    "# svc_random_1_poly = svc_bayesian_optimizer.optimize(n_calls=50, n_initial_points=50) \n",
    "\n",
    "# LOAD results from bayesian optimization\n",
    "svc_random_1_poly = pd.read_csv(f\"optimization/{task}/svc_random_1_poly\").drop(\"Unnamed: 0\", axis=1)\n",
    "\n",
    "svc_random_1_poly.sort_values(\"val_accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes about 2 minutes\n",
    "svc_random_1_poly_optimizer.best_confusion_matrix(svc_random_1_poly) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted SVC\n",
    "#### svc_weighted_bayesian_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_weighted_bayesian_0_bayesian_optimizer = BayesianOptimization(\n",
    "    model=SVC,\n",
    "    file_name=f\"{task}/svc_weighted_bayesian_0\", \n",
    "    model_params=[\n",
    "        Real(name=\"C\", low=0.1, high=4.0)\n",
    "    ],\n",
    "    datasets=datasets,\n",
    "    feature_groups=feature_groups,\n",
    ")\n",
    "\n",
    "# svc_weighted_bayesian_0 = svc_bayesian_optimizer.optimize() \n",
    "\n",
    "# LOAD results from bayesian optimization\n",
    "# svc_weighted_bayesian_0 = pd.read_csv(f\"optimization/{task}/svc_weighted_bayesian_0\").drop(\"Unnamed: 0\", axis=1)\n",
    "\n",
    "svc_weighted_bayesian_0.sort_values(\"val_accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForestClassifier\n",
    "\n",
    "For a random forest classifier we don't need to do any preprocessing. A decision tree based classifier is scale variant and has inbuild feature selection.\n",
    "\n",
    "#### max_depth\n",
    "\n",
    "turn following cell into python to re-run / edit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_random_forest_depth(datasets, file_path):\n",
    "    if exists(file_path):\n",
    "        return pd.read_csv(file_path).drop(\"Unnamed: 0\", axis=1)\n",
    "    max_depths = [depth for depth in range(3, 70)]\n",
    "    metric_values = []\n",
    "    x_train = np.array(datasets[\"train\"].drop(\"Y\", axis=1))\n",
    "    y_train = np.array(datasets[\"train\"][\"Y\"])\n",
    "    x_val = np.array(datasets[\"val\"].drop(\"Y\", axis=1))\n",
    "    y_val = np.array(datasets[\"val\"][\"Y\"])\n",
    "\n",
    "    with open(file_path, \"w\") as f:\n",
    "        f.write(f\",max_depth,val_accuracy\\n\")\n",
    "        for idx, max_depth in enumerate(max_depths):\n",
    "            rf = RandomForestClassifier(max_depth=max_depth, n_jobs=-1)\n",
    "            rf.fit(x_train, y_train)\n",
    "            y_pred = rf.predict(x_val)\n",
    "            acc = accuracy_score(y_val, y_pred)\n",
    "            print(\n",
    "                f\"Completed run {idx}/{len(max_depths)}: max_depth={max_depth}, accuracy={acc}\"\n",
    "            )\n",
    "            metric_values.append(acc)\n",
    "            f.write(f\"{idx}, {max_depth}, {acc}\\n\")\n",
    "\n",
    "    rf_results = pd.read_csv(file_path).drop(\"Unnamed: 0\", axis=1)\n",
    "    return rf_results\n",
    "\n",
    "\n",
    "rf_max_depth = train_random_forest_depth(\n",
    "    datasets=datasets, file_path=f\"optimization/{task}/rf_max_depth\"\n",
    ")\n",
    "\n",
    "rf_max_depth.sort_values(\"val_accuracy\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_parameter_metric(\n",
    "    metric_values=rf_max_depth[\"val_accuracy\"],\n",
    "    model_name=\"RandomForestClassifier\",\n",
    "    metric=\"validation accuracy\",\n",
    "    parameter=\"max_depth\",\n",
    "    param_values=rf_max_depth[\"max_depth\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array(datasets[\"train\"].drop(\"Y\", axis=1))\n",
    "y_train = np.array(datasets[\"train\"][\"Y\"])\n",
    "x_val = np.array(datasets[\"val\"].drop(\"Y\", axis=1))\n",
    "y_val = np.array(datasets[\"val\"][\"Y\"])\n",
    "\n",
    "best_rf = RandomForestClassifier(max_depth = 38, n_jobs=-1)\n",
    "best_rf.fit(x_train, y_train)\n",
    "y_pred = best_rf.predict(x_val)\n",
    "plot_confusion_matrix(y_val, y_pred, f\"RandomForestClassifier(max_depth=38)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "\n",
    "#### lr_bayesian_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_bayesian_optimizer = BayesianOptimization(\n",
    "    model=LogisticRegression,\n",
    "    file_name=f\"{task}/lr_bayesian_0\", \n",
    "    model_params=[\n",
    "        Categorical(name=\"penalty\", categories=[\"l1\", \"l2\"]),\n",
    "        Real(name=\"C\", low=0.1, high=4.0),\n",
    "        Categorical(name=\"solver\", categories=[\"saga\"]),\n",
    "        Categorical(name=\"n_jobs\", categories=[-1])\n",
    "    ],\n",
    "    datasets=datasets,\n",
    "    feature_groups=feature_groups,\n",
    ")\n",
    " \n",
    "#lr_results = lr_bayesian_optimizer.optimize() \n",
    "\n",
    "lr_results = pd.read_csv(f\"optimization/{task}/lr_bayesian_0\")\n",
    "lr_results.drop(\"Unnamed: 0\", axis=1).sort_values(\"val_accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_bayesian_optimizer.best_confusion_matrix(lr_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Steps\n",
    "\n",
    "- ~~Use Dendrogram [only continuous data] for feature selection (Jonna)~~\n",
    "- Feature selection method for discrete data (James)\n",
    "- ~~Baseline~~\n",
    "  - ~~DummyClassifier~~\n",
    "- Classical models\n",
    "  - ~~Random Forest (little feature selection) (Jonna)~~\n",
    "  - KNN + Bayesian Optimization (James)\n",
    "  - XGBoost + Bayesian Optimization (James)\n",
    "  - ~~SVC (Jonna)~~\n",
    "  - Linear Models (Jonna)\n",
    "  - Naive Bayes + Bayesian Optimization (James)\n",
    "  - [later] simple NN\n",
    "- Try giving weights to classes (will solve unbalanced data sets)\n",
    "- Get report working (James)\n",
    "- ~~Bayesian Optimization (Jonna)~~\n",
    "- apply the notebook to the other datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('astrazeneca')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "edc3225ee2ce0e02b9c5eb90a1bbc23c3d648c7d148b1d7ba33570195183ff0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
