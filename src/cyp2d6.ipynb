{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from os.path import exists \n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from feature_engine.selection import DropCorrelatedFeatures\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "from skopt.plots import plot_evaluations, plot_convergence, plot_objective\n",
    "from skopt.utils import dump, load\n",
    "\n",
    "from utils.data_preparation import *\n",
    "from utils.data_exploration import *\n",
    "from utils.training import *\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "task = \"cyp2d6\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Molecular Fingerprints\n",
    "\n",
    "Machine learning models almost always take arrays of numbers as their inputs. If we want to process molecules with them, we somehow need to represent each molecule as one or more arrays of numbers.\n",
    "\n",
    "Many (but not all) types of models require their inputs to have a fixed size. This can be a challenge for molecules, since different molecules have different numbers of atoms. If we want to use these types of models, we somehow need to represent variable sized molecules with fixed sized arrays.\n",
    "\n",
    "Fingerprints are designed to address these problems. A fingerprint is a fixed length array, where different elements indicate the presence of different features in the molecule. If two molecules have similar fingerprints, that indicates they contain many of the same features, and therefore will likely have similar chemistry.\n",
    "\n",
    "RDKit supports a particular type of fingerprint called an \"Extended Connectivity Fingerprint\", or \"ECFP\" for short. They also are sometimes called \"circular fingerprints\". The ECFP algorithm begins by classifying atoms based only on their direct properties and bonds. Each unique pattern is a feature. For example, \"carbon atom bonded to two hydrogens and two heavy atoms\" would be a feature, and a particular element of the fingerprint is set to 1 for any molecule that contains that feature. It then iteratively identifies new features by looking at larger circular neighborhoods. One specific feature bonded to two other specific features becomes a higher level feature, and the corresponding element is set for any molecule that contains it. This continues for a fixed number of iterations, most often two.\n",
    "\n",
    "source: [https://www.kaggle.com/code/shivanshuman/molecular-fingerprints]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Dataset + Data Cleaning\n",
    "\n",
    "In terms of data cleaning the following steps are performed:\n",
    "\n",
    "- Normalization of smiles strings before calculating descriptors and fingerprints\n",
    "  - Normalization includes the removal of metals in the molecule (<span style=\"color:cyan\">TODO</span> Why?)\n",
    "- Removal of small molecules\n",
    "  - For example: \n",
    "    - Molecules consisting of a single atom (<span style=\"color:cyan\">TODO</span> Why?)\n",
    "    - Molecules that are metals\n",
    "- Removing of NaN values by either removing the corresponding column or row. \n",
    "  - For molecular descriptors it doesn't make much sense to fill missing values with some default value or mean of the existing values\n",
    "\n",
    "\n",
    "In the first iteration we will focus on using the Morgan fingerprints. If there is time later we will explore other fingerprints and compare. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_preprocessing(task)\n",
    "# we only use Morgan fingerprints\n",
    "data = data.drop([\"MACCS_FP\", \"ATOMPAIR_FP\"], axis=1)\n",
    "data = select_druglike_molecules(data)\n",
    "# data = remove_small_molecules(data)\n",
    "\n",
    "# turn string of fingerprints into single features\n",
    "morgan_fingerprint_df = pd.DataFrame(\n",
    "    convert_strings_to_int_array(data[\"Morgan_FP\"].values), index=data.index\n",
    ")\n",
    "data = data.merge(morgan_fingerprint_df, left_index=True, right_index=True)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove missing values \n",
    "Since less than 1% of molecules have missing values we simply remove those molecules since using a default value doesn't make much sense for the shown descriptors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_nan = extract_null(data)\n",
    "print(\n",
    "    f\"There are {data_nan.shape[0]} ({data_nan.shape[0]/data.shape[0]*100:.2f}%) molecules and {data_nan.shape[1]-3} descriptors with missing values.\"\n",
    ")\n",
    "data = data.drop(data_nan.index)\n",
    "print(\"Data shape after dropping NaN samples:\", data.shape)\n",
    "data_nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Validation-Test split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data in train, val, test\n",
    "datasets = dataset_split(data.drop([\"Drug\", \"Drug_ID\", \"Morgan_FP\"], axis=1))\n",
    "# The descriptors include discrete and continuous data, distinguished by their dtype.\n",
    "feature_groups = get_feature_groups(datasets, morgan_fingerprint_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Exoploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pandas_profiling import ProfileReport\n",
    "# import pandas_profiling\n",
    "\n",
    "#datasets[\"train\"].profile_report( pool_size=1)\n",
    "#profile = ProfileReport(datasets[\"train\"], pool_size=1)\n",
    "#profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_counts(\n",
    "    [datasets[\"train\"][\"Y\"], datasets[\"val\"][\"Y\"], datasets[\"test\"][\"Y\"]],\n",
    "    suptitle=\"Distribution of the target label within each set\",\n",
    "    titles=[\"train\", \"validation\", \"test\"],\n",
    "    legend_title=\"CYP2C19 inhibition\",\n",
    "    kind=\"pie\",\n",
    ")\n",
    "datasets[\"train\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix of descriptors\n",
    "cor_matrix = datasets[\"train\"][feature_groups.continuous].corr()\n",
    "top_cor_matrix = cor_matrix.where(\n",
    "    np.triu(np.ones(cor_matrix.shape), k=1).astype(np.bool)\n",
    ")\n",
    "fig = px.imshow(\n",
    "    top_cor_matrix,\n",
    "    color_continuous_scale=\"RdBu_r\",\n",
    "    title=f\"{task} inhibition\\nDescriptor correlation\",\n",
    ")\n",
    "\n",
    "fig.write_html(f\"data/{task}/descriptor_correlation.html\")\n",
    "\n",
    "# violin plots\n",
    "feature_distributions(\n",
    "    data=datasets[\"train\"][[\"Y\"] + feature_groups.continuous],\n",
    "    features=feature_groups.continuous[10:14],\n",
    "    suptitle=\"Feature distributions given the target label using a KDE\",\n",
    "    task=f\"{task} inhibition\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discrete Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_distributions(\n",
    "    data=datasets[\"train\"][[\"Y\"] + feature_groups.discrete],\n",
    "    features=feature_groups.discrete[5:9],\n",
    "    kind=\"hist\",\n",
    "    suptitle=\"Feature Distributions given the target label\",\n",
    "    task=\"CYP2C19 inhibition\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are in total 208 different descriptors. Relevant descriptors for the task of predicting CYP inhibition need to be selected to reduce the number of input variables to the clasical machine learning algorithm. Feature selection can either be performed unsupervised (without knowledge of the target label) or supervised.\n",
    "\n",
    "**Note:** Some machine learning models have some form of feature selection inbuild, e.g. tree-based models. In those cases we don't perform feature selection upfront.\n",
    "\n",
    "### Variance Threshold\n",
    "\n",
    "Having a look at for example the number of radical electrons (NumRadicalElectrons). We can see that all datapoints in the dataset have a value of 0 (min=max=0.0). \n",
    "\n",
    "In the area of feature selection there is a method called **variance threshold**: Given a threshold all features with a variance below this threshold will be removed. (<span style=\"color:cyan\">TODO</span> Add better source; https://medium.com/nerd-for-tech/removing-constant-variables-feature-selection-463e2d6a30d9#:~:text=Variance%20Threshold%20is%20a%20feature,be%20used%20for%20unsupervised%20learning.)\n",
    "\n",
    "The default value is usually 0 (removing constant features as they obviously bring no additional information to our model). If the variance threshold is greater than zero but still small we are removing quasi-constant features. The arguments against using a variance greater than 0 say that you may be moving variables that, although they have low variance, might actually be extremely powerful in explaining your target (dependent) variable.\n",
    "\n",
    "For now, we are exploring which features are constant in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Features with 0 variance:\\n\")\n",
    "for index, n_unique in zip(\n",
    "    datasets[\"train\"].nunique(axis=0).index, datasets[\"train\"].nunique(axis=0)\n",
    "):\n",
    "    if n_unique == 1:\n",
    "        print(index)\n",
    "        \n",
    "print(\"\\n\", summarize_descriptors([\"NumRadicalElectrons\"]))\n",
    "datasets[\"train\"][\"NumRadicalElectrons\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Correlated Features\n",
    "\n",
    "As shown in the correlation matrix there are some feature groups in our dataset with high correlation. In order to escape the curse of dimensionality we want to remove features with a high correlation to other features - out of two features with high correlation only one remains. When features are collinear, permutating one feature will have little effect on the models performance because it can get the same information from a correlated feature. One way to handle multicollinear features is by performing hierarchical clustering on the Spearman rank-order correlations, picking a threshold, and keeping a single feature from each cluster. Source: https://scikit-learn.org/stable/auto_examples/inspection/plot_permutation_importance_multicollinear.html\n",
    "\n",
    "The y-axis of the following dendrogram is a measure of closeness of either individual data points or clusters. \n",
    "\n",
    "\n",
    "The idea of dropping highly correlated features is also applied by the following method: [DropCorrelatedFeatures](https://feature-engine.readthedocs.io/en/1.1.x/selection/DropCorrelatedFeatures.html) from the feature_engine. Here, features are removed on first found first removed basis, without any further insight using pearson correlation score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dendrogram(cor_matrix, level=7, color_threshold=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring DropCorrelatedFeatures\n",
    "\n",
    "drop_corr_features = DropCorrelatedFeatures(threshold=0.8)\n",
    "print(\n",
    "    \"Number of features before transformation:\",\n",
    "    datasets[\"train\"][feature_groups.continuous].shape[1],\n",
    ")\n",
    "reduced_continuous_data = drop_corr_features.fit_transform(\n",
    "    datasets[\"train\"][feature_groups.continuous]\n",
    ")\n",
    "print(\"Number of features after transformation:\", reduced_continuous_data.shape[1])\n",
    "\n",
    "# Correlation matrix of descriptors\n",
    "reduced_cor_matrix = reduced_continuous_data.corr()\n",
    "reduced_top_cor_matrix = reduced_cor_matrix.where(\n",
    "    np.triu(np.ones(reduced_cor_matrix.shape), k=1).astype(np.bool)\n",
    ")\n",
    "fig = px.imshow(\n",
    "    reduced_top_cor_matrix,\n",
    "    color_continuous_scale=\"RdBu_r\",\n",
    "    title=f\"{task} inhibition\\nDescriptor correlation after dropping highly correlated features\",\n",
    ")\n",
    "\n",
    "fig.write_html(f\"data/{task.lower()}/descriptor_correlation_pruned.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    **NOTE:** We don't do the following anymore.\n",
    "\n",
    "    ### Select Percentile\n",
    "\n",
    "    For discrete features and fingerprints we are using a mutual information statistical test and apply multivariate feature selection.\n",
    "\n",
    "    ## Dimensionality reduction\n",
    "\n",
    "    ### PCA\n",
    "\n",
    "    For continuous data we will perform a PCA to reduce the dimensionality of the features. Since PCA should only be applied to continuous data we will split our preprocessing pipeline into three parts:\n",
    "\n",
    "    1. Preprocessing of continuous descriptors\n",
    "    2. Preprocessing of discrete descriptors\n",
    "    3. Preprocessing of the fingerprint\n",
    "\n",
    "    See DataPreprocessing in utils/training.py for the exact preprocessing pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Normalization\n",
    "\n",
    "For continuous features and discrete descriptors we are using a MinMaxScaler. Since fingerprint features are binary we don't normalize them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "Use ```utils/training/BayesianOptimizer```. To do random search simply set ```n_calls=n_initial_points``` in ```self.optimize()```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummy Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_baseline(datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_0 = BayesianOptimization(\n",
    "    model=SVC,\n",
    "    file_name=f\"{task}/svc_0\", \n",
    "    model_params=[\n",
    "        Real(name=\"C\", low=0.1, high=4.0)\n",
    "    ],\n",
    "    fix_model_params={\"class_weight\": \"balanced\"},\n",
    "    datasets=datasets,\n",
    "    feature_groups=feature_groups,\n",
    "    main_metric=\"mcc\"\n",
    ")\n",
    "\n",
    "svc_0.optimize(n_calls=20) \n",
    "svc_0.pretty_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_columns = list(svc_0.results.filter(regex='val_'))\n",
    "# Position 0\n",
    "best_params_0 = svc_0.results.sort_values(\"val_mcc\", ascending=False).drop(metric_columns, axis=1).iloc[0]\n",
    "svc_0_y_pred, svc_0_y_pred_proba = svc_0.get_predictions(best_params_0)\n",
    "\n",
    "conf_matrix(datasets[\"val\"][\"Y\"], svc_0_y_pred, svc_0.file_loc)\n",
    "\n",
    "del svc_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomForestClassifier\n",
    "\n",
    "For a random forest classifier we don't need to do any preprocessing. A decision tree based classifier is scale invariant and has inbuild feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = datasets[\"train\"].drop(\"Y\", axis=1)\n",
    "y_train = datasets[\"train\"][\"Y\"]\n",
    "x_val = datasets[\"val\"].drop(\"Y\", axis=1)\n",
    "y_val = datasets[\"val\"][\"Y\"]\n",
    "\n",
    "# drop constant features\n",
    "preprocessing_pipe = DataPreprocessing(feature_groups, corr_threshold=1.0)\n",
    "preprocessing_pipe.fit(x_train, y_train)\n",
    "x_train_preprocessed = preprocessing_pipe.transform(x_train)\n",
    "x_val_preprocessed = preprocessing_pipe.transform(x_val)\n",
    "\n",
    "rf_max_depth = train_random_forest_depth(\n",
    "    f\"optimization/{task}/rf_max_depth\",\n",
    "    x_train_preprocessed,\n",
    "    x_val_preprocessed,\n",
    "    y_train,\n",
    "    y_val,\n",
    "    class_weight=\"balanced\"\n",
    ")\n",
    "\n",
    "metric_columns = list(rf_max_depth.filter(regex=\"val_\"))\n",
    "\n",
    "pretty_print_df(\n",
    "    rf_max_depth.sort_values(\"val_mcc\", ascending=False), subset=metric_columns, quantile=0.98\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_depth = int(rf_max_depth.sort_values(\"val_mcc\", ascending=False).iloc[0][\"max_depth\"])\n",
    "best_rf = RandomForestClassifier(max_depth = best_depth, class_weight=\"balanced\", n_jobs=-1)\n",
    "best_rf.fit(x_train_preprocessed, y_train)\n",
    "rf_best_y_pred = best_rf.predict(x_val_preprocessed)\n",
    "rf_best_y_pred_proba = best_rf.predict_proba(x_val_preprocessed)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_parameter_metric(\n",
    "    metric_values=rf_max_depth[\"val_f1\"],\n",
    "    model_name=\"RandomForestClassifier\",\n",
    "    metric=\"validation f1 score\",\n",
    "    parameter=\"max_depth\",\n",
    "    param_values=rf_max_depth[\"max_depth\"],\n",
    ")\n",
    "\n",
    "plot_parameter_metric(\n",
    "    metric_values=rf_max_depth[\"val_mcc\"],\n",
    "    model_name=\"RandomForestClassifier\",\n",
    "    metric=\"validation mcc\",\n",
    "    parameter=\"max_depth\",\n",
    "    param_values=rf_max_depth[\"max_depth\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix(y_val, rf_best_y_pred, f\"RandomForestClassifier(max_depth={best_depth})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_metric_curves(\n",
    "    {\n",
    "        \"RandomForestClassifier\": rf_best_y_pred_proba\n",
    "    },\n",
    "    datasets[\"val\"][\"Y\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_0 = BayesianOptimization(\n",
    "    model=LogisticRegression,\n",
    "    file_name=f\"{task}/lr_0\", \n",
    "    model_params=[\n",
    "        Categorical(name=\"penalty\", categories=[\"l1\", \"l2\"]),\n",
    "        Real(name=\"C\", low=0.1, high=4.0),\n",
    "    ],\n",
    "    fix_model_params={\"solver\": \"saga\", \"n_jobs\": -1, \"class_weight\":\"balanced\"},\n",
    "    datasets=datasets,\n",
    "    feature_groups=feature_groups,\n",
    "    main_metric=\"mcc\"\n",
    ")\n",
    " \n",
    "lr_0.optimize(n_calls=20) \n",
    "lr_0.pretty_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_columns = list(lr_0.results.filter(regex='val_'))\n",
    "print(\"Filtered metric columns\", metric_columns)\n",
    "# Position 0\n",
    "best_params_0 = lr_0.results.sort_values(\"val_mcc\", ascending=False).drop(metric_columns, axis=1).iloc[0]\n",
    "lr_0_best_y_pred, lr_0_best_y_pred_proba = lr_0.get_predictions(best_params_0)\n",
    "\n",
    "conf_matrix(datasets[\"val\"][\"Y\"], lr_0_best_y_pred, lr_0.file_loc)\n",
    "\n",
    "del lr_0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CatBoost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_negative = datasets[\"train\"][\"Y\"][datasets[\"train\"][\"Y\"] == 0].count()\n",
    "n_positive = datasets[\"train\"][\"Y\"][datasets[\"train\"][\"Y\"] == 1].count()\n",
    "\n",
    "catboost_0 = BayesianOptimization(\n",
    "    model=CatBoostClassifier,\n",
    "    file_name=f\"{task}/catboost_0\",\n",
    "    model_params=[\n",
    "        Integer(name=\"max_depth\", low=4, high=12),\n",
    "        Real(name=\"l2_leaf_reg\", low=2., high=10.),\n",
    "        # Categorical(name=\"boosting_type\", categories=[\"Ordered\", \"Plain\"]),\n",
    "        Real(name=\"scale_pos_weight\",low=max(1,n_negative/n_positive-1), high=10),\n",
    "    ],\n",
    "    fix_model_params={\n",
    "        \"verbose\": 0,\n",
    "    },\n",
    "    datasets=datasets,\n",
    "    feature_groups=feature_groups,\n",
    "    main_metric=\"mcc\",\n",
    ")\n",
    "\n",
    "catboost_0.optimize(n_calls=70)\n",
    "catboost_0.pretty_results(quantile=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_columns = list(catboost_0.results.filter(regex='val_'))\n",
    "print(\"Filtered metric columns\", metric_columns)\n",
    "# Position 0\n",
    "best_params_0 = catboost_0.results.sort_values(\"val_mcc\", ascending=False).drop(metric_columns, axis=1).iloc[0]\n",
    "catboost_0_best_y_pred, catboost_0_best_y_pred_proba = catboost_0.get_predictions(best_params_0)\n",
    "\n",
    "conf_matrix(datasets[\"val\"][\"Y\"], catboost_0_best_y_pred, catboost_0.file_loc)\n",
    "\n",
    "del catboost_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_0 = BayesianOptimization(\n",
    "    model=KNeighborsClassifier,\n",
    "    file_name=f\"{task}/knn_0\", \n",
    "    model_params=[\n",
    "        Integer(name=\"n_neighbors\", low=3, high=60),\n",
    "    ],\n",
    "    datasets=datasets,\n",
    "    feature_groups=feature_groups,\n",
    "    preprocessing_params=None,\n",
    "    main_metric=\"mcc\",\n",
    "    fix_model_params={\"weights\":\"distance\"}\n",
    ")\n",
    "\n",
    "knn_0.optimize(n_calls=20)\n",
    "knn_0.pretty_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_columns = list(knn_0.results.filter(regex='val_'))\n",
    "print(\"Filtered metric columns\", metric_columns)\n",
    "\n",
    "best_params_0 = list(knn_0.results.sort_values(\"val_mcc\", ascending=False).drop(metric_columns, axis=1).iloc[0])\n",
    "best_params_0[0] = int(best_params_0[0])\n",
    "knn_0_best_y_pred, knn_0_best_y_pred_proba = knn_0.get_predictions(best_params_0)\n",
    "conf_matrix(datasets[\"val\"][\"Y\"], knn_0_best_y_pred, knn_0.file_loc)\n",
    "\n",
    "del knn_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_negative = datasets[\"train\"][\"Y\"][datasets[\"train\"][\"Y\"] == 0].count()\n",
    "n_positive = datasets[\"train\"][\"Y\"][datasets[\"train\"][\"Y\"] == 1].count()\n",
    "\n",
    "xgboost_0 = BayesianOptimization(\n",
    "    model=XGBClassifier,\n",
    "    file_name=f\"{task}/xgboost_0\",\n",
    "    model_params=[\n",
    "        Integer(name=\"max_depth\", low=5, high=50),\n",
    "        Real(name=\"eta\", low=0.01, high=0.2),\n",
    "        Real(name=\"subsample\", low=0.5, high=1),\n",
    "        Real(name=\"scale_pos_weight\", low=max(1, n_negative / n_positive - 1), high=10),\n",
    "        Real(name=\"colsample_bytree\", low=0.5, high=1.0),\n",
    "        Real(name=\"lambda\", low=0.5, high=4.0),\n",
    "    ],\n",
    "    fix_model_params={\"objective\": \"binary:logistic\", \"eval_metric\": \"aucpr\"},\n",
    "    datasets=datasets,\n",
    "    feature_groups=feature_groups,\n",
    "    main_metric=\"mcc\",\n",
    ")\n",
    "\n",
    "xgboost_0.optimize(n_calls=70)\n",
    "xgboost_0.pretty_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_columns = list(xgboost_0.results.filter(regex='val_'))\n",
    "print(\"Filtered metric columns\", metric_columns)\n",
    "# Position 0\n",
    "best_params_0 = list(xgboost_0.results.sort_values(\"val_mcc\", ascending=False).drop(metric_columns, axis=1).iloc[0])\n",
    "best_params_0[0] = int(best_params_0[0])\n",
    "xgboost_0_best_y_pred, xgboost_0_best_y_pred_proba = xgboost_0.get_predictions(best_params_0)\n",
    "\n",
    "conf_matrix(datasets[\"val\"][\"Y\"], xgboost_0_best_y_pred, xgboost_0.file_loc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_metric_curves(\n",
    "    {\n",
    "        \"XGBoost\" : xgboost_0_best_y_pred_proba\n",
    "    },\n",
    "    datasets[\"val\"][\"Y\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(f\"optimization/{task}/mcc/best.csv\", comment=\"#\").sort_values(\"val_mcc\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix(datasets[\"val\"][\"Y\"], (xgboost_0_best_y_pred_proba >= 0.15), xgboost_0.file_loc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix(datasets[\"val\"][\"Y\"], (xgboost_0_best_y_pred_proba >= 0.25), xgboost_0.file_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('astrazeneca')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "edc3225ee2ce0e02b9c5eb90a1bbc23c3d648c7d148b1d7ba33570195183ff0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
